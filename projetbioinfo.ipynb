{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HATANBAATAR VAN Erkhembileg & MICHEL Victor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\erkhe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import re\n",
    "from gensim.models import KeyedVectors\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    data = []\n",
    "    current_id = ''\n",
    "    for line in lines:\n",
    "        if line.startswith('###'):\n",
    "            current_id = line.strip().lstrip('###')\n",
    "        elif line.strip():\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                category, text = parts\n",
    "                data.append({'id': current_id, 'category': category, 'text': text})\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = parse_data('train.txt')\n",
    "df_test = parse_data('test.txt')\n",
    "\n",
    "df_train['text_cleaned'] = df_train['text'].apply(clean_text)\n",
    "df_test['text_cleaned'] = df_test['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erkhe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['arent', 'couldnt', 'didnt', 'doesnt', 'dont', 'hadnt', 'hasnt', 'havent', 'isnt', 'mightnt', 'mustnt', 'neednt', 'shant', 'shes', 'shouldnt', 'shouldve', 'thatll', 'wasnt', 'werent', 'wont', 'wouldnt', 'youd', 'youll', 'youre', 'youve'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification modèle bag of words + régression logistique :\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  BACKGROUND       0.58      0.49      0.53      2663\n",
      " CONCLUSIONS       0.69      0.66      0.67      4426\n",
      "     METHODS       0.81      0.89      0.85      9751\n",
      "   OBJECTIVE       0.70      0.58      0.63      2377\n",
      "     RESULTS       0.82      0.83      0.83     10276\n",
      "\n",
      "    accuracy                           0.77     29493\n",
      "   macro avg       0.72      0.69      0.70     29493\n",
      "weighted avg       0.77      0.77      0.77     29493\n",
      "\n",
      "Validation croisée modèle bag of words, accuracy : 0.7700890791529936\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=3000, preprocessor=clean_text)\n",
    "X_train = vectorizer.fit_transform(df_train['text_cleaned'])\n",
    "y_train = df_train['category']\n",
    "X_test = vectorizer.transform(df_test['text_cleaned'])\n",
    "y_test = df_test['category']\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=1000, solver='sag', tol=0.1)\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_predictions = lr_model.predict(X_test)\n",
    "\n",
    "print(\"Classification modèle bag of words + régression logistique :\\n\")\n",
    "print(classification_report(y_test, lr_predictions))\n",
    "\n",
    "cv_scores = cross_val_score(lr_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(\"Validation croisée modèle bag of words, accuracy :\", cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification pour le modèle avec embeddings pré-entraînés :\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  BACKGROUND       0.60      0.48      0.53      2663\n",
      " CONCLUSIONS       0.68      0.70      0.69      4426\n",
      "     METHODS       0.82      0.87      0.85      9751\n",
      "   OBJECTIVE       0.65      0.61      0.63      2377\n",
      "     RESULTS       0.84      0.84      0.84     10276\n",
      "\n",
      "    accuracy                           0.78     29493\n",
      "   macro avg       0.72      0.70      0.71     29493\n",
      "weighted avg       0.77      0.78      0.77     29493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = 'BioWordVec_PubMed_MIMICIII_d200.vec.bin'\n",
    "embedding_model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "\n",
    "def sentence_vector(sentence, model):\n",
    "    words = sentence.split()\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    else:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "\n",
    "X_train_embedded = np.array([sentence_vector(text, embedding_model) for text in df_train['text_cleaned']])\n",
    "X_test_embedded = np.array([sentence_vector(text, embedding_model) for text in df_test['text_cleaned']])\n",
    "\n",
    "lr_model_embedded = LogisticRegression(max_iter=1000)\n",
    "lr_model_embedded.fit(X_train_embedded, y_train)\n",
    "embedded_predictions = lr_model_embedded.predict(X_test_embedded)\n",
    "\n",
    "print(\"Classification pour le modèle avec embeddings pré-entraînés :\\n\")\n",
    "print(classification_report(y_test, embedded_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
